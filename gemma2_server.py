import socket
import json
import threading
import time
import requests
import re

class OptimizedGemma2Server(object):
    def __init__(self, port=8888, pepper_port=8889):
        self.port = port
        self.pepper_port = pepper_port
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.pepper_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.is_running = False
        
        # Configuration Ollama optimis√©e
        self.ollama_url = "http://192.168.1.17:11434/api/generate"
        self.model_name = "gemma2:9b"
        
        # Buffer conversation
        self.current_conversation = []
        self.conversation_active = False
        
        # Optimisations conversationnelles
        self.conversation_context = []
        self.max_context_length = 8

    def start_server(self):
        try:
            self.sock.bind(('0.0.0.0', self.port))
            self.is_running = True

            print(f"üß† SERVEUR GEMMA2:9B ULTRA-OPTIMIS√â")
            print(f"üì° R√©ception Pepper: port {self.port}")
            print(f"üöÄ Ollama Gemma2:9B sur RTX 4070")
            print(f"‚ö° R√©ponses conversationnelles ultra-rapides")

            # V√©rification Ollama
            self.check_ollama_status()

            self.receiver_thread = threading.Thread(target=self.receive_voice_data)
            self.receiver_thread.daemon = True
            self.receiver_thread.start()

        except Exception as e:
            print(f"‚ùå Erreur d√©marrage: {e}")

    def check_ollama_status(self):
        """V√©rification qu'Ollama et Gemma2 sont pr√™ts"""
        try:
            print(f"üîé Test de connexion √† Ollama : {self.ollama_url}")
            response = requests.post(self.ollama_url, json={
                "model": self.model_name,
                "prompt": "Bonjour",
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "max_tokens": 50
                }
            }, timeout=10)

            print(f"R√©ponse connexion Ollama HTTP Code : {response.status_code}")
            if response.status_code == 200:
                print(f"‚úÖ Gemma2:9B pr√™t et optimis√©\nR√©ponse : {response.json()}")
            else:
                print(f"‚ö†Ô∏è  Probl√®me Ollama: {response.status_code} {response.text}")

        except Exception as e:
            print(f"‚ùå Ollama non accessible: {e}")
            print("üí° Lancez: ollama serve")

    def receive_voice_data(self):
        while self.is_running:
            try:
                data, addr = self.sock.recvfrom(4096)
                print(f"\nüì• Re√ßu UDP depuis {addr} : {data!r}")
                voice_data = json.loads(data.decode('utf-8'))
                print(f"‚û°Ô∏è Donn√©es re√ßues : {voice_data}")
                self.process_voice_data(voice_data, addr)
            except Exception as e:
                print(f"‚ùå Erreur r√©ception : {e}")

    def process_voice_data(self, voice_data, sender_addr):
        try:
            msg_type = voice_data.get('type')
            print(f"üîé Type de message : {msg_type}")
            
            if msg_type == 'conversation_start':
                self.conversation_active = True
                self.current_conversation = []
                print(f"üé§ NOUVELLE CONVERSATION")
                
            elif msg_type == 'speech_chunk' and self.conversation_active:
                self.current_conversation.append(voice_data)
                words = voice_data.get('words')
                print(f"üìù Chunk re√ßu: {words}")
                
            elif msg_type == 'conversation_end':
                print(f"‚úÖ TRAITEMENT GEMMA2:9B (fin de phrase d√©tect√©e)")
                
                # Log les donn√©es re√ßues pour debug
                print(f"--- Donn√©es conversation compl√®te :")
                print(json.dumps(self.current_conversation, indent=2, ensure_ascii=False))
                print(f"--- conversation_text (direct) re√ßu: {voice_data.get('conversation_text', '<absent>')}")
                
                # Ajout imm√©diat du texte si fourni par le message direct
                if 'conversation_text' in voice_data:
                    self.current_conversation.append(voice_data)
                
                if self.current_conversation:
                    llm_response = self.process_with_gemma2()
                    self.send_response_to_pepper(llm_response, sender_addr[0])
                self.conversation_active = False
                
        except Exception as e:
            print(f"‚ùå Erreur traitement : {e}")

    def process_with_gemma2(self):
        """Traitement ultra-optimis√© avec Gemma2:9B"""
        try:
            # Extraire le texte, supporte conversation_text direct
            if self.current_conversation and isinstance(self.current_conversation[-1], dict) and 'conversation_text' in self.current_conversation[-1]:
                conversation_text = self.current_conversation[-1]['conversation_text']
            else:
                # fallback mots chunks si jamais utile
                detected_words = []
                for chunk in self.current_conversation:
                    words = chunk.get('words')
                    if words and isinstance(words, str):
                        detected_words.append(words)
                conversation_text = " ".join(detected_words).strip()
            
            print(f"üß† GEMMA2 PROMPT envoy√© : '{conversation_text}'")

            if not conversation_text.strip():
                print("üö® conversation_text vide, envoi r√©ponse d√©faut")
                return "Je n'ai pas bien compris. Pouvez-vous r√©p√©ter ?"

            system_prompt = (
                "Tu es un assistant professionnel dans un √©v√©nement. "
                "R√©ponds de mani√®re concise, claire et professionnelle. "
                "Maximum 2 phrases courtes. Sois naturel et aidant."
            )

            context = ""
            if self.conversation_context:
                context = "\n".join([f"H: {h}\nA: {a}" for h, a in self.conversation_context[-3:]])
                context = f"\nContexte r√©cent:\n{context}\n"

            full_prompt = (
                f"{system_prompt}{context}\n"
                f"Humain: {conversation_text}\n"
                f"Assistant:"
            )

            print(f"üß† PROMPT FINAL:\n{full_prompt}\n")

            # Appel √† Ollama
            start_time = time.time()
            response = requests.post(self.ollama_url, json={
                "model": self.model_name,
                "prompt": full_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1,
                    "num_ctx": 4096,
                    "num_predict": 100,
                    "stop": ["Humain:", "H:"]
                }
            }, timeout=30)

            processing_time = time.time() - start_time

            if response.status_code == 200:
                result = response.json()
                print(f"üß† R√©ponse RAW Ollama JSON:\n{result}\n")
                gemma_response = result.get('response', '').strip()
                gemma_response = self.clean_response(gemma_response)
                self.conversation_context.append((conversation_text, gemma_response))
                if len(self.conversation_context) > self.max_context_length:
                    self.conversation_context.pop(0)
                print(f"‚ö° GEMMA2 R√âPOND ({processing_time:.2f}s): '{gemma_response}'")
                return gemma_response
            else:
                print(f"‚ùå Erreur Ollama: {response.status_code} {response.text}")
                return "Je rencontre un probl√®me technique. Pouvez-vous r√©essayer ?"

        except requests.Timeout:
            print("‚è±Ô∏è Timeout Ollama !")
            return "D√©sol√©, je r√©fl√©chis trop lentement. Pouvez-vous r√©p√©ter ?"
        except Exception as e:
            print(f"‚ùå Erreur Gemma2: {e}")
            return "Je n'ai pas pu traiter votre demande. Reformulez s'il vous pla√Æt."

    def clean_response(self, response):
        response = re.sub(r'^(Assistant|A):\s*', '', response)
        response = re.sub(r'^(R√©ponse|Response):\s*', '', response)
        response = response.replace('*', '').replace('#', '')
        sentences = response.split('. ')
        if len(sentences) > 2:
            response = '. '.join(sentences[:2]) + '.'
        return response.strip()

    def send_response_to_pepper(self, llm_response, pepper_ip):
        try:
            response_data = {
                'type': 'llm_response',
                'text': llm_response,
                'timestamp': time.time(),
                'model': 'gemma2:9b'
            }
            json_data = json.dumps(response_data, ensure_ascii=False)
            payload = json_data.encode('utf-8')
            
            # ‚úÖ CORRECTION : Envoie vers WSL, pas vers Pepper physique
            wsl_ip = "172.25.227.111"  # Ton IP WSL
            print(f"üü¢ Envoi r√©ponse LLM √† {wsl_ip}:{self.pepper_port} (WSL)")
            self.pepper_sock.sendto(payload, (wsl_ip, self.pepper_port))
            
        except Exception as e:
            print(f"‚ùå Erreur envoi: {e}")

    def stop_server(self):
        self.is_running = False
        self.sock.close()
        self.pepper_sock.close()
        print("üõë Serveur Gemma2 arr√™t√©")

if __name__ == "__main__":
    print("üöÄ SERVEUR GEMMA2:9B ULTRA-OPTIMIS√â")
    print("   ‚ö° RTX 4070 + i9 = Conversations ultra-rapides")
    print("   üéØ Sp√©cialis√© √©v√©nementiel professionnel")
    print("   üß† Gemma2:9B > Llama 3.1 8B pour conversations")
    
    server = OptimizedGemma2Server()
    
    try:
        server.start_server()
        print("\n‚úÖ Gemma2:9B pr√™t pour conversations professionnelles!")
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\nüõë Arr√™t Gemma2...")
    finally:
        server.stop_server()
