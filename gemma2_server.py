import socket
import json
import threading
import time
import requests
import re
import os


class OptimizedGemma2Server(object):
    def __init__(self, port=8888, pepper_port=8889):
        self.port = port
        self.pepper_port = pepper_port
        self.sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.pepper_sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        self.is_running = False
        
        # Configuration Ollama ultra-optimis√©e
        self.ollama_url = "http://192.168.1.17:11434/api/generate"
        self.model_name = "gemma2:9b"
        
        # Buffer conversation
        self.current_conversation = []
        self.conversation_active = False
        
        # Optimisations conversationnelles - R√âDUIT
        self.conversation_context = []
        self.max_context_length = 2  # R√©duit de 8 √† 2


    def start_server(self):
        try:
            self.sock.bind(('0.0.0.0', self.port))
            self.is_running = True

            print(f"üß† SERVEUR GEMMA2:9B ULTRA-OPTIMIS√â")
            print(f"üì° R√©ception Pepper: port {self.port}")
            print(f"üöÄ Ollama Gemma2:9B sur RTX 4070")
            print(f"‚ö° Streaming + optimisations GPU activ√©es")

            # Warm-up et v√©rification Ollama
            self.check_ollama_status()

            self.receiver_thread = threading.Thread(target=self.receive_voice_data)
            self.receiver_thread.daemon = True
            self.receiver_thread.start()

        except Exception as e:
            print(f"‚ùå Erreur d√©marrage: {e}")


    def check_ollama_status(self):
        """Warm-up du mod√®le + v√©rification"""
        try:
            print(f"üî• Warm-up Gemma2:9B...")
            # Requ√™te de warm-up pour charger le mod√®le en GPU
            response = requests.post(self.ollama_url, json={
                "model": self.model_name,
                "prompt": "Hi",
                "stream": False,
                "options": {"num_predict": 1}
            }, timeout=15)

            print(f"R√©ponse warm-up HTTP Code : {response.status_code}")
            if response.status_code == 200:
                print(f"‚úÖ Gemma2:9B charg√© et optimis√© en GPU")
            else:
                print(f"‚ö†Ô∏è  Probl√®me warm-up: {response.status_code} {response.text}")

        except Exception as e:
            print(f"‚ùå Ollama non accessible: {e}")
            print("üí° Lancez: ollama serve")


    def receive_voice_data(self):
        while self.is_running:
            try:
                data, addr = self.sock.recvfrom(4096)
                print(f"\nüì• Re√ßu UDP depuis {addr} : {data!r}")
                voice_data = json.loads(data.decode('utf-8'))
                print(f"‚û°Ô∏è Donn√©es re√ßues : {voice_data}")
                self.process_voice_data(voice_data, addr)
            except Exception as e:
                print(f"‚ùå Erreur r√©ception : {e}")


    def process_voice_data(self, voice_data, sender_addr):
        try:
            msg_type = voice_data.get('type')
            print(f"üîé Type de message : {msg_type}")
            
            if msg_type == 'conversation_start':
                self.conversation_active = True
                self.current_conversation = []
                print(f"üé§ NOUVELLE CONVERSATION")
                
            elif msg_type == 'speech_chunk' and self.conversation_active:
                self.current_conversation.append(voice_data)
                words = voice_data.get('words')
                print(f"üìù Chunk re√ßu: {words}")
                
            elif msg_type == 'conversation_end':
                print(f"‚úÖ TRAITEMENT GEMMA2:9B (fin de phrase d√©tect√©e)")
                
                # Ajout imm√©diat du texte si fourni par le message direct
                if 'conversation_text' in voice_data:
                    self.current_conversation.append(voice_data)
                
                if self.current_conversation:
                    llm_response = self.process_with_gemma2()
                    self.send_response_to_pepper(llm_response, sender_addr[0])
                self.conversation_active = False
                
        except Exception as e:
            print(f"‚ùå Erreur traitement : {e}")


    def process_with_gemma2(self):
        """Traitement STREAMING ultra-optimis√© avec Gemma2:9B"""
        try:
            # Extraire le texte
            if self.current_conversation and isinstance(self.current_conversation[-1], dict) and 'conversation_text' in self.current_conversation[-1]:
                conversation_text = self.current_conversation[-1]['conversation_text']
            else:
                detected_words = []
                for chunk in self.current_conversation:
                    words = chunk.get('words')
                    if words and isinstance(words, str):
                        detected_words.append(words)
                conversation_text = " ".join(detected_words).strip()
            
            print(f"üß† GEMMA2 PROMPT envoy√© : '{conversation_text}'")

            if not conversation_text.strip():
                return "Je n'ai pas bien compris. Pouvez-vous r√©p√©ter ?"

            # Prompt ultra-concis
            system_prompt = "Assistant pro √©v√©nementiel. 1 phrase max, direct."
            
            # Context ultra-r√©duit (1 seul √©change pr√©c√©dent)
            context = ""
            if self.conversation_context:
                h, a = self.conversation_context[-1]  # SEULEMENT LE DERNIER
                context = f"\nDernier √©change:\nH: {h}\nA: {a}\n"

            full_prompt = f"{system_prompt}{context}\nHumain: {conversation_text}\nAssistant:"
            print(f"üß† PROMPT FINAL:\n{full_prompt}\n")

            # Appel √† Ollama EN STREAMING
            start_time = time.time()
            response = requests.post(self.ollama_url, json={
                "model": self.model_name,
                "prompt": full_prompt,
                "stream": True,  # STREAMING ACTIV√â
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "repeat_penalty": 1.1,
                    "num_predict": 35,     # R√âDUIT √Ä 35 TOKENS
                    "num_ctx": 2048,       # CONTEXTE R√âDUIT
                    "stop": ["Humain:", "H:", ".", "!"]  # STOP AGRESSIF
                }
            }, timeout=30, stream=True)

            # Lecture streaming ligne par ligne
            gemma_response = ""
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode('utf-8'))
                    delta = chunk.get("response", "")
                    gemma_response += delta

            processing_time = time.time() - start_time
            
            # Nettoyage final
            gemma_response = self.clean_response(gemma_response.strip())
            self.conversation_context.append((conversation_text, gemma_response))
            if len(self.conversation_context) > self.max_context_length:
                self.conversation_context.pop(0)
                
            print(f"‚ö° GEMMA2 STREAMING ({processing_time:.2f}s): '{gemma_response}'")
            return gemma_response

        except requests.Timeout:
            print("‚è±Ô∏è Timeout Ollama !")
            return "D√©sol√©, je r√©fl√©chis trop lentement."
        except Exception as e:
            print(f"‚ùå Erreur Gemma2: {e}")
            return "Je n'ai pas pu traiter votre demande."


    def clean_response(self, response):
        response = re.sub(r'^(Assistant|A):\s*', '', response)
        response = re.sub(r'^(R√©ponse|Response):\s*', '', response)
        response = response.replace('*', '').replace('#', '')
        # Garde seulement la premi√®re phrase
        sentences = response.split('. ')
        if sentences:
            response = sentences[0] + '.'
        return response.strip()


    def send_response_to_pepper(self, llm_response, pepper_ip):
        try:
            response_data = {
                'type': 'llm_response',
                'text': llm_response,
                'timestamp': time.time(),
                'model': 'gemma2:9b'
            }
            json_data = json.dumps(response_data, ensure_ascii=False)
            payload = json_data.encode('utf-8')
            
            wsl_ip = "172.25.227.111"
            print(f"üü¢ Envoi r√©ponse LLM √† {wsl_ip}:{self.pepper_port} (WSL)")
            self.pepper_sock.sendto(payload, (wsl_ip, self.pepper_port))
            
        except Exception as e:
            print(f"‚ùå Erreur envoi: {e}")


    def stop_server(self):
        self.is_running = False
        self.sock.close()
        self.pepper_sock.close()
        print("üõë Serveur Gemma2 arr√™t√©")


if __name__ == "__main__":
    # Optimisations GPU pour Ollama
    os.environ["OLLAMA_FLASH_ATTENTION"] = "1"
    os.environ["OLLAMA_KV_CACHE_TYPE"] = "q4_0"
    os.environ["OLLAMA_NUM_PARALLEL"] = "1"
    
    print("üöÄ SERVEUR GEMMA2:9B ULTRA-OPTIMIS√â")
    print("   ‚ö° RTX 4070 + Streaming + GPU optimis√©")
    print("   üéØ R√©ponses < 2.5s apr√®s warm-up")
    print("   üß† Gemma2:9B streaming + cache optimis√©")
    
    server = OptimizedGemma2Server()
    
    try:
        server.start_server()
        print("\n‚úÖ Gemma2:9B pr√™t avec optimisations streaming!")
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\nüõë Arr√™t Gemma2...")
    finally:
        server.stop_server()
